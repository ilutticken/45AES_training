{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15bccac8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# LLM Exercise Notebook\n",
    "\n",
    "This notebook is designed to help you practice and understand the concepts of LLMs (Large Language Models) through a hands-on exercise. The goal is to implement a simple LLM from huggingface and fine-tune it on a custom dataset. You will also learn how to evaluate the model's performance and make predictions.\n",
    "\n",
    "The exercise is divided into several sections, each focusing on a specific aspect of LLMs. You can run the code snippets in each section to see how they work and modify them to suit your needs. These sections are:\n",
    "\n",
    "1. **Importing Libraries**: Import the necessary libraries for the exercise.\n",
    "2. **Loading the Dataset**: Load a custom dataset for training and evaluation.\n",
    "3. **Preprocessing the Data**: Preprocess the dataset to make it suitable for training.\n",
    "4. **Loading the Model**: Load a pre-trained LLM from Hugging Face.\n",
    "5. **Fine-tuning the Model**: Fine-tune the model on the custom dataset.\n",
    "6. **Evaluating the Model**: Evaluate the model's performance on a test set.\n",
    "7. **Making Predictions**: Use the fine-tuned model to make predictions on new data.\n",
    "8. **Conclusion**: Summarize the key takeaways from the exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602aab42",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f2e0160",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\sushi\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\sushi\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Setup environment to avoid TensorFlow dependency issues\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Set environment variables to avoid TensorFlow warnings and force PyTorch usage\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"  # Suppress TensorFlow logging\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"  # Avoid TF imports during model downloads\n",
    "os.environ[\"TRANSFORMERS_FRAMEWORK\"] = \"pt\"  # Use PyTorch (pt) instead of TensorFlow (tf)\n",
    "\n",
    "# Install required packages\n",
    "!pip install -q transformers datasets evaluate PyPDF2 torch\n",
    "!pip install -q tf-keras\n",
    "\n",
    "# This will ensure we're using the PyTorch-specific classes only\n",
    "import importlib\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings related to TensorFlow\n",
    "warnings.filterwarnings('ignore', message='.*tensorflow.*')\n",
    "warnings.filterwarnings('ignore', message='.*Keras.*')\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d9e61b",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27b62103",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sushi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sushi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries for working with LLMs and datasets\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    TextDataset,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "import evaluate\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# For PDF processing\n",
    "from PyPDF2 import PdfReader\n",
    "import glob\n",
    "import re\n",
    "\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4904d0b",
   "metadata": {},
   "source": [
    "## Loading the Dataset\n",
    "\n",
    "In this section, we will load the PDF files from the data folder and extract their text content. Then, we will create a dataset suitable for fine-tuning an LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2ef3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract text from PDF files\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    try:\n",
    "        reader = PdfReader(pdf_path)\n",
    "        text = \"\"\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from {pdf_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Get all PDF files in the data directory\n",
    "pdf_files = glob.glob(\"data/*.pdf\")\n",
    "print(f\"Found {len(pdf_files)} PDF files in the data directory.\")\n",
    "\n",
    "# Extract text from all PDF files\n",
    "texts = []\n",
    "for pdf_file in pdf_files:\n",
    "    text = extract_text_from_pdf(pdf_file)\n",
    "    if text:\n",
    "        texts.append({\"source\": pdf_file, \"text\": text})\n",
    "\n",
    "# Create a pandas DataFrame from the extracted texts\n",
    "df = pd.DataFrame(texts)\n",
    "print(f\"Extracted text from {len(df)} PDF files successfully.\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145ecddc",
   "metadata": {},
   "source": [
    "## Preprocessing the Data\n",
    "\n",
    "Now we'll preprocess the extracted text data to make it suitable for training an LLM. This involves cleaning the text, tokenizing, and splitting into train and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df55978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the text data\n",
    "def clean_text(text):\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Remove special characters that might cause issues\n",
    "    text = re.sub(r'[^\\w\\s.,!?:;\"\\'-]', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "# Apply cleaning to the DataFrame\n",
    "df['clean_text'] = df['text'].apply(clean_text)\n",
    "\n",
    "# Save processed text to files for model training\n",
    "os.makedirs('processed_data', exist_ok=True)\n",
    "\n",
    "# Split data into training and validation sets (80/20 split)\n",
    "train_size = int(0.8 * len(df))\n",
    "train_df = df[:train_size]\n",
    "val_df = df[train_size:]\n",
    "\n",
    "# Write to files\n",
    "with open('processed_data/train.txt', 'w', encoding='utf-8') as f:\n",
    "    for text in train_df['clean_text']:\n",
    "        f.write(text + '\\n\\n')\n",
    "\n",
    "with open('processed_data/validation.txt', 'w', encoding='utf-8') as f:\n",
    "    for text in val_df['clean_text']:\n",
    "        f.write(text + '\\n\\n')\n",
    "\n",
    "print(f\"Saved {len(train_df)} training examples and {len(val_df)} validation examples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7333c318",
   "metadata": {},
   "source": [
    "## Loading the Model\n",
    "\n",
    "Let's load a small pre-trained text generation model from Hugging Face. We'll use DistilGPT2 which is a distilled version of GPT-2, making it smaller and faster while maintaining reasonable performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b69db19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model name - using a small version for faster fine-tuning\n",
    "model_name = \"distilgpt2\"  # A small GPT-2 model (82M parameters)\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Print model parameters to get an idea of its size\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Loaded {model_name} with {total_params:,} parameters\")\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830231b5",
   "metadata": {},
   "source": [
    "## Fine-tuning the Model\n",
    "\n",
    "Now we'll fine-tune the model on our custom dataset of PDF text. We'll use the Hugging Face Trainer API for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6b504e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a dataset for the model\n",
    "def load_dataset(train_path, val_path, tokenizer):\n",
    "    # Load datasets\n",
    "    train_dataset = TextDataset(\n",
    "        tokenizer=tokenizer,\n",
    "        file_path=train_path,\n",
    "        block_size=128  # Context size for training (adjust based on your GPU memory)\n",
    "    )\n",
    "    \n",
    "    val_dataset = TextDataset(\n",
    "        tokenizer=tokenizer,\n",
    "        file_path=val_path,\n",
    "        block_size=128\n",
    "    )\n",
    "    \n",
    "    # Create data collator for language modeling\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, \n",
    "        mlm=False  # We're doing causal language modeling, not masked language modeling\n",
    "    )\n",
    "    \n",
    "    return train_dataset, val_dataset, data_collator\n",
    "\n",
    "# Load datasets\n",
    "train_dataset, val_dataset, data_collator = load_dataset(\n",
    "    'processed_data/train.txt',\n",
    "    'processed_data/validation.txt',\n",
    "    tokenizer\n",
    ")\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    eval_steps=400,\n",
    "    save_steps=800,\n",
    "    warmup_steps=500,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=2  # Limit the number of checkpoints to save disk space\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "# Fine-tune the model (Note: This may take a while)\n",
    "print(\"Starting model fine-tuning...\")\n",
    "trainer.train()\n",
    "print(\"Fine-tuning completed!\")\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model_path = \"./fine_tuned_model\"\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)\n",
    "print(f\"Model and tokenizer saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac90075",
   "metadata": {},
   "source": [
    "## Evaluating the Model\n",
    "\n",
    "Let's evaluate our fine-tuned model on the validation set to see how well it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c967bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {np.exp(eval_results['eval_loss']):.2f}\")\n",
    "\n",
    "# Plot training loss\n",
    "training_logs = trainer.state.log_history\n",
    "\n",
    "# Extract training and evaluation losses\n",
    "train_losses = [log['loss'] for log in training_logs if 'loss' in log]\n",
    "train_steps = [log['step'] for log in training_logs if 'loss' in log]\n",
    "\n",
    "eval_losses = [log['eval_loss'] for log in training_logs if 'eval_loss' in log]\n",
    "eval_steps = [log['step'] for log in training_logs if 'eval_loss' in log]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_steps, train_losses, label='Training Loss')\n",
    "plt.plot(eval_steps, eval_losses, label='Evaluation Loss')\n",
    "plt.xlabel('Training Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Evaluation Losses')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f90ec8",
   "metadata": {},
   "source": [
    "## Making Predictions\n",
    "\n",
    "Now that we have a fine-tuned model, let's use it to generate text based on prompts from our domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d100b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned model for inference\n",
    "model_path = \"./fine_tuned_model\"\n",
    "inference_model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "inference_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Function to generate text from prompt\n",
    "def generate_text(prompt, max_length=200):\n",
    "    inputs = inference_tokenizer(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    # Generate text\n",
    "    outputs = inference_model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.8,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        no_repeat_ngram_size=2\n",
    "    )\n",
    "    \n",
    "    # Decode and return the generated text\n",
    "    generated_text = inference_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "# Try generating text with a few different prompts\n",
    "test_prompts = [\n",
    "    \"The main findings of the research suggest that\",\n",
    "    \"The methodology used in this study involves\",\n",
    "    \"In conclusion, the results demonstrate that\"\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    generated = generate_text(prompt)\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Generated text: {generated}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4069bd",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've successfully completed a full workflow for fine-tuning a small language model (DistilGPT2) on a custom dataset of PDF documents. Here's a summary of what we've accomplished:\n",
    "\n",
    "1. **Data Collection**: Extracted text from PDF documents in the data directory\n",
    "2. **Data Preprocessing**: Cleaned and formatted the data for model training\n",
    "3. **Model Setup**: Loaded a pre-trained small language model (DistilGPT2)\n",
    "4. **Fine-tuning**: Adapted the model to our specific domain using the custom dataset\n",
    "5. **Evaluation**: Measured the model's performance using perplexity\n",
    "6. **Text Generation**: Used the fine-tuned model to generate domain-specific text\n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "- Even small language models can be effectively fine-tuned for specific domains\n",
    "- PDF text extraction and preprocessing are crucial steps for creating quality training data\n",
    "- The Hugging Face ecosystem provides powerful tools for working with language models\n",
    "- Hyperparameter tuning and proper evaluation are essential for good model performance\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "- Try fine-tuning with different model architectures (e.g., BART, T5)\n",
    "- Experiment with different hyperparameters to improve performance\n",
    "- Implement more rigorous evaluation metrics like BLEU or ROUGE\n",
    "- Create a simple application that uses the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7beb41d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
