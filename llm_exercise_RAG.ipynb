{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad664401",
   "metadata": {},
   "source": [
    "# LLM Exercise with RAG\n",
    "\n",
    "This notebook demonstrates how to use a language model with a retrieval-augmented generation (RAG) approach. The goal is to retrieve relevant documents from a knowledge base and use them to generate a response to a user query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133bd508",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d950aa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries to pull a model from Hugging Face and set it up with RAG\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "from transformers import RagTokenizer, RagRetriever, RagSequenceForGeneration\n",
    "from transformers import RagConfig, RagTokenForGeneration\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
    "from transformers import BlenderbotTokenizer, BlenderbotForConditionalGeneration\n",
    "\n",
    "# Import libraries for efficient small LLMs\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "\n",
    "# For RAG components\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "\n",
    "# For file handling\n",
    "import os\n",
    "import glob\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6a46dd",
   "metadata": {},
   "source": [
    "## Small LLMs for Local Inference\n",
    "\n",
    "For running inference locally with reasonable performance, there are several excellent small LLMs to consider:\n",
    "\n",
    "1. **Phi-2** (2.7B parameters) - Microsoft's model with excellent reasoning for its size\n",
    "2. **TinyLlama** (1.1B parameters) - Extremely compact with decent performance\n",
    "3. **Mistral-7B-Instruct** - Great performance/size trade-off with 7B parameters\n",
    "4. **Llama-2-7B** - Good general-purpose model that can be quantized further\n",
    "\n",
    "These models can be loaded with quantization (4-bit or 8-bit) to reduce memory requirements while maintaining most of the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b8117f",
   "metadata": {},
   "source": [
    "## Load a Small LLM for Local Inference\n",
    "\n",
    "Below are examples of how to load different small LLMs with quantization for efficient inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93b81315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bccbc7f09b9442b581ec1558ea2a6028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e58fb404c85480fb0d1fcca89b93d46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TinyLlama model loaded successfully (CPU mode)!\n"
     ]
    }
   ],
   "source": [
    "# Choose and load your preferred model\n",
    "# Uncomment the model you want to use\n",
    "\n",
    "# Option 1: Phi-2 (2.7B) - Best performance/size ratio\n",
    "# model, tokenizer = load_phi2_model()\n",
    "# print(\"Phi-2 model loaded successfully!\")\n",
    "\n",
    "# Modified function to load TinyLlama without GPU requirements\n",
    "def load_tinyllama_cpu():\n",
    "    \"\"\"Load TinyLlama (1.1B parameters) for CPU inference without quantization\"\"\"\n",
    "    model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        device_map=\"cpu\",\n",
    "        torch_dtype=torch.float32,  # Use regular precision instead of quantization\n",
    "    )\n",
    "    return model, tokenizer\n",
    "\n",
    "# Option 2: TinyLlama (1.1B) - Smallest size\n",
    "model, tokenizer = load_tinyllama_cpu()\n",
    "print(\"TinyLlama model loaded successfully (CPU mode)!\")\n",
    "\n",
    "# Option 3: Mistral (7B) - Best overall performance\n",
    "# model, tokenizer = load_mistral_model()\n",
    "# print(\"Mistral-7B model loaded successfully!\")\n",
    "\n",
    "# Create a text generation pipeline\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.15\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474e0d49",
   "metadata": {},
   "source": [
    "## Set Up RAG with Local Documents\n",
    "\n",
    "Now we'll set up a Retrieval-Augmented Generation (RAG) system using your local PDF text files.\n",
    "This will allow the model to generate answers based on your document collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d477dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 26 text files in data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48210bc7dfb2411c87d3646816c22adf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading documents:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 26 documents\n"
     ]
    }
   ],
   "source": [
    "# Load text files from the data directory\n",
    "def load_documents(directory=\"data\"):\n",
    "    \"\"\"Load all text files from the specified directory\"\"\"\n",
    "    txt_files = glob.glob(os.path.join(directory, \"*.txt\"))\n",
    "    print(f\"Found {len(txt_files)} text files in {directory}\")\n",
    "    \n",
    "    documents = []\n",
    "    for file_path in tqdm(txt_files, desc=\"Loading documents\"):\n",
    "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            content = f.read()\n",
    "            documents.append({\"content\": content, \"source\": os.path.basename(file_path)})\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# Load and preprocess documents\n",
    "documents = load_documents()\n",
    "print(f\"Loaded {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0bad06a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f2548c3f03d4b9fb4551fbd70d4f337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Splitting documents:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 17312 chunks from 26 documents\n"
     ]
    }
   ],
   "source": [
    "# Split documents into chunks for processing\n",
    "def split_documents(documents, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"Split documents into manageable chunks\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    \n",
    "    chunks = []\n",
    "    for doc in tqdm(documents, desc=\"Splitting documents\"):\n",
    "        doc_chunks = text_splitter.split_text(doc[\"content\"])\n",
    "        for chunk in doc_chunks:\n",
    "            chunks.append({\"content\": chunk, \"source\": doc[\"source\"]})\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Split documents into manageable chunks\n",
    "chunks = split_documents(documents)\n",
    "print(f\"Created {len(chunks)} chunks from {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a51a8ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Training Machine\\AppData\\Local\\Temp\\ipykernel_9976\\1429758578.py:5: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38e4a7cf515e400fbae722ad96d27cfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "495df4278ab443da854bbd530304fc93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f66d089bd0e84f4b8a41fe929e2a267b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7fccd9e815c485a8f732735d8b0e986",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb24e3b414c94f4caae7d670b724957b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c85789e4ae9424187837c98f717d147",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3df6113bddb4f82987c0dd180345d5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1531b307790f4826b9ba2070e94a6dfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55518c6dbfea4436b1a02dda0ff405b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58a212f58ad54f9c8bd18e3ab90ee0ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "532ea740b34b483daa6406d5f665ee99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load a small embedding model\n",
    "def setup_embeddings_and_vectorstore(chunks):\n",
    "    \"\"\"Set up embeddings and vector storage for document retrieval\"\"\"\n",
    "    # Use a small, efficient embedding model\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    \n",
    "    # Extract chunk texts and their metadata\n",
    "    texts = [chunk[\"content\"] for chunk in chunks]\n",
    "    metadatas = [{\"source\": chunk[\"source\"]} for chunk in chunks]\n",
    "    \n",
    "    # Create FAISS vector store\n",
    "    vectorstore = FAISS.from_texts(\n",
    "        texts=texts,\n",
    "        embedding=embeddings,\n",
    "        metadatas=metadatas\n",
    "    )\n",
    "    \n",
    "    return vectorstore\n",
    "\n",
    "# Set up vector store for retrieval\n",
    "vectorstore = setup_embeddings_and_vectorstore(chunks)\n",
    "print(\"Vector store created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f111242",
   "metadata": {},
   "source": [
    "## RAG Inference with Local LLM\n",
    "\n",
    "Now we can use our local LLM with the RAG setup to answer questions based on the document collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6aad6fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_query(query, num_chunks=3, generator=generator):\n",
    "    \"\"\"Query the RAG system with a user question\"\"\"\n",
    "    # Retrieve relevant chunks\n",
    "    retrieved_docs = vectorstore.similarity_search(query, k=num_chunks)\n",
    "    retrieved_text = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "    sources = list(set([doc.metadata[\"source\"] for doc in retrieved_docs]))\n",
    "    \n",
    "    # Create context-enriched prompt\n",
    "    prompt = f\"\"\"Context information is below.\n",
    "---------------------\n",
    "{retrieved_text}\n",
    "---------------------\n",
    "Given the context information and no prior knowledge, answer the following question: {query}\n",
    "\"\"\"\n",
    "    \n",
    "    # Generate response using the local LLM\n",
    "    response = generator(prompt, max_new_tokens=512)[0][\"generated_text\"]\n",
    "    \n",
    "    # Remove the prompt from the response\n",
    "    response = response.replace(prompt, \"\")\n",
    "    \n",
    "    return {\n",
    "        \"response\": response,\n",
    "        \"sources\": sources\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b9303f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Training Machine\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Training Machine\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How do I set up a Cisco ASA 5505 firewall?\n",
      "\n",
      "Answer:\n",
      "\n",
      "Answer: To set up a Cisco ASA 5505 firewall, follow these steps:\n",
      "\n",
      "1. Download and install the latest version of the Cisco ASA Firewall Manager software from the Cisco website.\n",
      "\n",
      "2. Open the ASA Firewall Manager console by clicking on the \"Start\" button and selecting \"Run.\"\n",
      "\n",
      "3. In the console, navigate to the \"Administration\" tab and click on \"Add/Remove Devices.\"\n",
      "\n",
      "4. Select \"Add Device\" and enter the details for the new device, including its name, IP address, and username or password.\n",
      "\n",
      "5. Click \"OK\" to add the device to the list of devices.\n",
      "\n",
      "6. Once the device has been added, click on it to open the device configuration screen.\n",
      "\n",
      "7. Under the \"General\" tab, specify the name of the device, such as \"MyDevice,\" and enter any necessary settings, such as the IP address and subnet mask.\n",
      "\n",
      "8. For the \"Security\" tab, select the appropriate security policy for the device, such as \"Basic\" or \"Advanced.\"\n",
      "\n",
      "9. Click \"Save\" to save the changes to the device configuration.\n",
      "\n",
      "10. Return to the main ASA Firewall Manager console and verify that the device is now configured correctly.\n",
      "\n",
      "Sources:\n",
      "- CISCO ASA_5505 User manual.txt\n",
      "- merged_text.txt\n"
     ]
    }
   ],
   "source": [
    "# Example query\n",
    "query = \"How do I set up a Cisco ASA 5505 firewall?\"\n",
    "result = rag_query(query)\n",
    "\n",
    "print(\"Question:\", query)\n",
    "print(\"\\nAnswer:\")\n",
    "print(result[\"response\"])\n",
    "print(\"\\nSources:\")\n",
    "for source in result[\"sources\"]:\n",
    "    print(f\"- {source}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893460f1",
   "metadata": {},
   "source": [
    "## Interactive RAG Query Interface\n",
    "\n",
    "Use the cell below to query your document collection interactively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7441a6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0769420237114bc78d212ff0d2617c52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Query:', layout=Layout(width='80%'), placeholder='Type your question here...')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec24a4f46c31409982bdf5e3b3e80fb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='primary', description='Submit', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "095a8ff8f2de4d0c8da13a247770c917",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import widgets\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Create input widget\n",
    "query_input = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Type your question here...',\n",
    "    description='Query:',\n",
    "    layout=widgets.Layout(width='80%')\n",
    ")\n",
    "\n",
    "# Create output widget\n",
    "output = widgets.Output()\n",
    "\n",
    "# Define submit button callback\n",
    "def on_submit_clicked(b):\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        print(\"Processing query...\")\n",
    "        result = rag_query(query_input.value)\n",
    "        print(\"\\n\\nAnswer:\")\n",
    "        print(result[\"response\"])\n",
    "        print(\"\\nSources:\")\n",
    "        for source in result[\"sources\"]:\n",
    "            print(f\"- {source}\")\n",
    "\n",
    "# Create and configure submit button\n",
    "submit_button = widgets.Button(\n",
    "    description='Submit',\n",
    "    button_style='primary'\n",
    ")\n",
    "submit_button.on_click(on_submit_clicked)\n",
    "\n",
    "# Display the interface\n",
    "display(query_input, submit_button, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d201763f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
