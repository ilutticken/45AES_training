{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad664401",
   "metadata": {},
   "source": [
    "# LLM Exercise with RAG\n",
    "\n",
    "This notebook demonstrates how to use a language model with a retrieval-augmented generation (RAG) approach. The goal is to retrieve relevant documents from a knowledge base and use them to generate a response to a user query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133bd508",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "976bd422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in c:\\users\\training machine\\anaconda3\\lib\\site-packages (1.6.0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in c:\\users\\training machine\\anaconda3\\lib\\site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\training machine\\anaconda3\\lib\\site-packages (from accelerate) (24.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\training machine\\anaconda3\\lib\\site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\training machine\\anaconda3\\lib\\site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\training machine\\anaconda3\\lib\\site-packages (from accelerate) (2.7.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in c:\\users\\training machine\\anaconda3\\lib\\site-packages (from accelerate) (0.31.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\training machine\\anaconda3\\lib\\site-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\training machine\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\training machine\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.6.1)\n",
      "Requirement already satisfied: requests in c:\\users\\training machine\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\training machine\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\training machine\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\training machine\\anaconda3\\lib\\site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\training machine\\anaconda3\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\training machine\\anaconda3\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\training machine\\anaconda3\\lib\\site-packages (from torch>=2.0.0->accelerate) (75.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\training machine\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\training machine\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.21.0->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\training machine\\anaconda3\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\training machine\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\training machine\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\training machine\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\training machine\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d950aa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries to pull a model from Hugging Face and set it up with RAG\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "from transformers import RagTokenizer, RagRetriever, RagSequenceForGeneration\n",
    "from transformers import RagConfig, RagTokenForGeneration\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
    "from transformers import BlenderbotTokenizer, BlenderbotForConditionalGeneration\n",
    "\n",
    "# Import libraries for efficient small LLMs\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "\n",
    "# For quantization to reduce memory footprint\n",
    "import bitsandbytes as bnb\n",
    "import accelerate\n",
    "\n",
    "# For RAG components\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "\n",
    "# For file handling\n",
    "import os\n",
    "import glob\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6a46dd",
   "metadata": {},
   "source": [
    "## Small LLMs for Local Inference\n",
    "\n",
    "For running inference locally with reasonable performance, there are several excellent small LLMs to consider:\n",
    "\n",
    "1. **Phi-2** (2.7B parameters) - Microsoft's model with excellent reasoning for its size\n",
    "2. **TinyLlama** (1.1B parameters) - Extremely compact with decent performance\n",
    "3. **Mistral-7B-Instruct** - Great performance/size trade-off with 7B parameters\n",
    "4. **Llama-2-7B** - Good general-purpose model that can be quantized further\n",
    "\n",
    "These models can be loaded with quantization (4-bit or 8-bit) to reduce memory requirements while maintaining most of the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b8117f",
   "metadata": {},
   "source": [
    "## Load a Small LLM for Local Inference\n",
    "\n",
    "Below are examples of how to load different small LLMs with quantization for efficient inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5567bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and tokenizer\n",
    "def load_phi2_model():\n",
    "    \"\"\"Load Microsoft's Phi-2 (2.7B parameters) - excellent small model\"\"\"\n",
    "    model_id = \"microsoft/phi-2\"\n",
    "    \n",
    "    # Load with 4-bit quantization to reduce memory usage\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        device_map=\"auto\",\n",
    "        load_in_4bit=True,  # Use 4-bit quantization\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    return model, tokenizer\n",
    "\n",
    "def load_tinyllama_model():\n",
    "    \"\"\"Load TinyLlama (1.1B parameters) - extremely compact\"\"\"\n",
    "    model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        device_map=\"auto\",\n",
    "        load_in_8bit=True,  # Use 8-bit quantization\n",
    "    )\n",
    "    return model, tokenizer\n",
    "\n",
    "def load_mistral_model():\n",
    "    \"\"\"Load Mistral-7B-Instruct - very efficient for its size\"\"\"\n",
    "    model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        device_map=\"auto\",\n",
    "        load_in_4bit=True,  # Use 4-bit quantization for larger models\n",
    "    )\n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "93b81315",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Using a `device_map` or `tp_plan` requires `accelerate`. You can install it with `pip install accelerate`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Choose and load your preferred model\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Uncomment the model you want to use\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Option 1: Phi-2 (2.7B) - Best performance/size ratio\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m load_phi2_model()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPhi-2 model loaded successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Option 2: TinyLlama (1.1B) - Smallest size\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# model, tokenizer = load_tinyllama_model()\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# print(\"TinyLlama model loaded successfully!\")\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m \n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Create a text generation pipeline\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[20], line 8\u001b[0m, in \u001b[0;36mload_phi2_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Load with 4-bit quantization to reduce memory usage\u001b[39;00m\n\u001b[0;32m      7\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id)\n\u001b[1;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m      9\u001b[0m     model_id,\n\u001b[0;32m     10\u001b[0m     device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     11\u001b[0m     load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# Use 4-bit quantization\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     13\u001b[0m )\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model, tokenizer\n",
      "File \u001b[1;32mc:\\Users\\Training Machine\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:571\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mconfig_class \u001b[38;5;241m==\u001b[39m config\u001b[38;5;241m.\u001b[39msub_configs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    570\u001b[0m         config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_text_config()\n\u001b[1;32m--> 571\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    572\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    573\u001b[0m     )\n\u001b[0;32m    574\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    575\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    576\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    577\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Training Machine\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py:279\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    281\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[1;32mc:\\Users\\Training Machine\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py:4139\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   4137\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeepSpeed Zero-3 is not compatible with passing a `device_map`.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   4138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[1;32m-> 4139\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   4140\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing a `device_map` or `tp_plan` requires `accelerate`. You can install it with `pip install accelerate`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4141\u001b[0m         )\n\u001b[0;32m   4143\u001b[0m \u001b[38;5;66;03m# handling bnb config from kwargs, remove after `load_in_{4/8}bit` deprecation.\u001b[39;00m\n\u001b[0;32m   4144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_in_4bit \u001b[38;5;129;01mor\u001b[39;00m load_in_8bit:\n",
      "\u001b[1;31mValueError\u001b[0m: Using a `device_map` or `tp_plan` requires `accelerate`. You can install it with `pip install accelerate`"
     ]
    }
   ],
   "source": [
    "# Choose and load your preferred model\n",
    "# Uncomment the model you want to use\n",
    "\n",
    "# Option 1: Phi-2 (2.7B) - Best performance/size ratio\n",
    "model, tokenizer = load_phi2_model()\n",
    "print(\"Phi-2 model loaded successfully!\")\n",
    "\n",
    "# Option 2: TinyLlama (1.1B) - Smallest size\n",
    "# model, tokenizer = load_tinyllama_model()\n",
    "# print(\"TinyLlama model loaded successfully!\")\n",
    "\n",
    "# Option 3: Mistral (7B) - Best overall performance\n",
    "# model, tokenizer = load_mistral_model()\n",
    "# print(\"Mistral-7B model loaded successfully!\")\n",
    "\n",
    "# Create a text generation pipeline\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.15\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be19bb27",
   "metadata": {},
   "source": [
    "## Model Comparison and Requirements\n",
    "\n",
    "| Model | Parameters | Quantized Size | Min RAM | VRAM (GPU) | Performance | Best For |\n",
    "|-------|------------|----------------|---------|------------|------------|----------|\n",
    "| Phi-2 | 2.7B | ~1.5-2GB | 4GB | 2GB | Very Good | Balanced use cases |\n",
    "| TinyLlama | 1.1B | ~700MB | 2GB | 1GB | Moderate | Resource-constrained devices |\n",
    "| Mistral-7B | 7B | ~3.5-4GB | 8GB | 4GB | Excellent | When performance matters more |\n",
    "\n",
    "Notes:\n",
    "- 4-bit quantization reduces memory by ~75% with minor quality loss\n",
    "- 8-bit quantization reduces memory by ~50% with minimal quality loss\n",
    "- CPU inference is possible but much slower; a GPU is recommended"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474e0d49",
   "metadata": {},
   "source": [
    "## Set Up RAG with Local Documents\n",
    "\n",
    "Now we'll set up a Retrieval-Augmented Generation (RAG) system using your local PDF text files.\n",
    "This will allow the model to generate answers based on your document collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d477dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load text files from the data directory\n",
    "def load_documents(directory=\"data\"):\n",
    "    \"\"\"Load all text files from the specified directory\"\"\"\n",
    "    txt_files = glob.glob(os.path.join(directory, \"*.txt\"))\n",
    "    print(f\"Found {len(txt_files)} text files in {directory}\")\n",
    "    \n",
    "    documents = []\n",
    "    for file_path in tqdm(txt_files, desc=\"Loading documents\"):\n",
    "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            content = f.read()\n",
    "            documents.append({\"content\": content, \"source\": os.path.basename(file_path)})\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# Load and preprocess documents\n",
    "documents = load_documents()\n",
    "print(f\"Loaded {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bad06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split documents into chunks for processing\n",
    "def split_documents(documents, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"Split documents into manageable chunks\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    \n",
    "    chunks = []\n",
    "    for doc in tqdm(documents, desc=\"Splitting documents\"):\n",
    "        doc_chunks = text_splitter.split_text(doc[\"content\"])\n",
    "        for chunk in doc_chunks:\n",
    "            chunks.append({\"content\": chunk, \"source\": doc[\"source\"]})\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Split documents into manageable chunks\n",
    "chunks = split_documents(documents)\n",
    "print(f\"Created {len(chunks)} chunks from {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51a8ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a small embedding model\n",
    "def setup_embeddings_and_vectorstore(chunks):\n",
    "    \"\"\"Set up embeddings and vector storage for document retrieval\"\"\"\n",
    "    # Use a small, efficient embedding model\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    \n",
    "    # Extract chunk texts and their metadata\n",
    "    texts = [chunk[\"content\"] for chunk in chunks]\n",
    "    metadatas = [{\"source\": chunk[\"source\"]} for chunk in chunks]\n",
    "    \n",
    "    # Create FAISS vector store\n",
    "    vectorstore = FAISS.from_texts(\n",
    "        texts=texts,\n",
    "        embedding=embeddings,\n",
    "        metadatas=metadatas\n",
    "    )\n",
    "    \n",
    "    return vectorstore\n",
    "\n",
    "# Set up vector store for retrieval\n",
    "vectorstore = setup_embeddings_and_vectorstore(chunks)\n",
    "print(\"Vector store created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f111242",
   "metadata": {},
   "source": [
    "## RAG Inference with Local LLM\n",
    "\n",
    "Now we can use our local LLM with the RAG setup to answer questions based on the document collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aad6fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_query(query, num_chunks=3, generator=generator):\n",
    "    \"\"\"Query the RAG system with a user question\"\"\"\n",
    "    # Retrieve relevant chunks\n",
    "    retrieved_docs = vectorstore.similarity_search(query, k=num_chunks)\n",
    "    retrieved_text = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "    sources = list(set([doc.metadata[\"source\"] for doc in retrieved_docs]))\n",
    "    \n",
    "    # Create context-enriched prompt\n",
    "    prompt = f\"\"\"Context information is below.\n",
    "---------------------\n",
    "{retrieved_text}\n",
    "---------------------\n",
    "Given the context information and no prior knowledge, answer the following question: {query}\n",
    "\"\"\"\n",
    "    \n",
    "    # Generate response using the local LLM\n",
    "    response = generator(prompt, max_new_tokens=512)[0][\"generated_text\"]\n",
    "    \n",
    "    # Remove the prompt from the response\n",
    "    response = response.replace(prompt, \"\")\n",
    "    \n",
    "    return {\n",
    "        \"response\": response,\n",
    "        \"sources\": sources\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9303f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example query\n",
    "query = \"How do I set up a Cisco ASA 5505 firewall?\"\n",
    "result = rag_query(query)\n",
    "\n",
    "print(\"Question:\", query)\n",
    "print(\"\\nAnswer:\")\n",
    "print(result[\"response\"])\n",
    "print(\"\\nSources:\")\n",
    "for source in result[\"sources\"]:\n",
    "    print(f\"- {source}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893460f1",
   "metadata": {},
   "source": [
    "## Interactive RAG Query Interface\n",
    "\n",
    "Use the cell below to query your document collection interactively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7441a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import widgets\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Create input widget\n",
    "query_input = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Type your question here...',\n",
    "    description='Query:',\n",
    "    layout=widgets.Layout(width='80%')\n",
    ")\n",
    "\n",
    "# Create output widget\n",
    "output = widgets.Output()\n",
    "\n",
    "# Define submit button callback\n",
    "def on_submit_clicked(b):\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        print(\"Processing query...\")\n",
    "        result = rag_query(query_input.value)\n",
    "        print(\"\\n\\nAnswer:\")\n",
    "        print(result[\"response\"])\n",
    "        print(\"\\nSources:\")\n",
    "        for source in result[\"sources\"]:\n",
    "            print(f\"- {source}\")\n",
    "\n",
    "# Create and configure submit button\n",
    "submit_button = widgets.Button(\n",
    "    description='Submit',\n",
    "    button_style='primary'\n",
    ")\n",
    "submit_button.on_click(on_submit_clicked)\n",
    "\n",
    "# Display the interface\n",
    "display(query_input, submit_button, output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
